{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will show the following operations for a `Tile` object:\n",
    " \n",
    " - Creating a tile object from a shape file.\n",
    " - Loading data into the tile object for both world clim and cru-jra data.\n",
    " - Saving the tile object to a folder.\n",
    "\n",
    "When a tile is created, it is empty, but has an extent as determined from the \n",
    "shape file used to create the tile. \n",
    "\n",
    "When loading data, it is assumed that the world clim and cru data sources have\n",
    "extents fully encompasing the tile. When the data is loaded it is clipped to the\n",
    "extents of the tile, optionally including a buffer beyond the tile extents\n",
    "(reccomened; helps with warping and reprojection).\n",
    "\n",
    "When a tile is saved, all the datasets that the tile holds are saved to disk in \n",
    "an appropriately dimensioned netcdf file. Additionally there is a manifest file\n",
    "which will have metadata (tile index, extents, resolution, crs, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development...\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from temds.datasources import worldclim\n",
    "from temds.datasources import crujra\n",
    "\n",
    "from temds import tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Creating a `Tile` object\n",
    "\n",
    "Here we setup a few parameters defining the tile we want to create. This includes\n",
    "the:\n",
    "  - Tile \"index\" (horizontal and vertical offsets from lower left corner). \n",
    "  The shape file should be annotated with these horizontal and vertical indices \n",
    "  at attributes for each tile.\n",
    "  - Year range to work with (typically all the data you have, but for \n",
    "    development and testing you can make it shorter).\n",
    "\n",
    "Then we read the bounds for the tile from the shape file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1901\n",
    "end_year = 2023\n",
    "c_tile = (0, 8)\n",
    "tile_index = gpd.read_file('working/tile_index_annotated.shp')\n",
    "hdx = tile_index['H'] == c_tile[0]\n",
    "vdx = tile_index['V'] == c_tile[1]\n",
    "bounds = tile_index[vdx & hdx].bounds\n",
    "minx, maxx, miny, maxy = bounds[['minx','maxx','miny','maxy']].iloc[0]\n",
    "minx, maxx, miny, maxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Next we create a tile object. At this point the tile object is \"empty\" - there is no data loaded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile = tile.Tile(c_tile, bounds, 4000, tile_index.crs, buffer_px=20)\n",
    "mytile.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile.buffer_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile.extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Loading data to the `Tile` object\n",
    "\n",
    "Once we have created a `Tile` object we can load some data to it. \n",
    "\n",
    "Before we can import (load) the data into the `Tile` object, we need to open and create the datasets. For this tile, we will be using WorldClim and CRU-JRA data.\n",
    "\n",
    "## WorldClim\n",
    "\n",
    "The WorldClim data is relatively straightforward - it is a single NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_arctic = worldclim.WorldClim('working/02-arctic/worldclim/worldclim-arctic.nc')\n",
    "print(f\"The CRS for the WorldClim dataset is: {wc_arctic.dataset.rio.crs}\")\n",
    "\n",
    "mytile.import_normalized('worldclim', wc_arctic, flip_y = True)\n",
    "\n",
    "mytile.data['worldclim']\n",
    "#del(wc_arctic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WorldClim dimensions: {mytile.data['worldclim'].dims}\")\n",
    "print(f\"Resolution: {mytile.resolution}\")\n",
    "tile_x_size = (mytile.extent['maxx']-mytile.extent['minx'])/mytile.resolution\n",
    "tile_y_size = (mytile.extent['maxy']-mytile.extent['miny'])/mytile.resolution\n",
    "print(f\"Tile Size: {tile_x_size.values} x {tile_y_size.values} pixels\")\n",
    "print(f\"Tile Buffer Pixels: {mytile.buffer_pixels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now we can plot and look at the data to double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots (1,1, dpi=100)\n",
    "\n",
    "im = axes.imshow(mytile.data['worldclim']['tmax'].data[0], origin='lower')\n",
    "fig.colorbar(im, ax=axes)\n",
    "axes.set_title('wc example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## CRU-JRA\n",
    "The CRU-JRA data is a little more complicated as it is a series of NetCDF files.\n",
    "\n",
    "First we can do some poking around to see what files we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = sorted(list(Path('working/02-arctic/cru-jra-fixed/').glob('*.nc')))\n",
    "print(f\"Found {len(i)} files.\")\n",
    "print(i[0])\n",
    "print(\"...\")\n",
    "print(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "It is up to the user to make sure they have enough files. You must have a continuous range covering your `start_year` through your `end_year` that you set above.\n",
    "\n",
    "This will create a list of `crujra.AnnualDaily` objects. The data is resampled from 6 hour to daily when these objects are created.\n",
    "\n",
    "> (~4 minutes and 18GB RAM on 8 core machine with 32GB RAM and SSD)\n",
    "\n",
    "### Resample from 6h to daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens cru files, reads into memory, then we have a list of cru objects at \n",
    "# the end...\n",
    "annual_list = []\n",
    "file_list = sorted(list(Path('working/02-arctic/cru-jra-25/').glob('*.nc')))\n",
    "print(f\"culling by year: {start_year} - {end_year}\")\n",
    "for cru_file in file_list:\n",
    "    year = int(cru_file.name.split('.')[-4])\n",
    "    if year >= start_year and year <= end_year:\n",
    "        temp = crujra.AnnualDaily(year, cru_file, verbose=False, force_aoi_to='tmax', aoi_nodata=np.nan)\n",
    "        # temp.reproject(tile_index.crs.to_wkt())\n",
    "        annual_list.append(temp)\n",
    "    else:\n",
    "        continue\n",
    "print(len(annual_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now we have a list of `crujra.AnnualDaily` objects, and we can turn that list into a `crujra.AnnualTimeSeries` object. This `AnnualTimeSeries` object has some special features like being able to index by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cru_arctic_ts = crujra.AnnualTimeSeries(annual_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We can plot this data to confirm that it is the expected shape. The data should be monthly and have all variables present. The data should cover the entire acrtic and is un-projected, so it doesn't look great, but you get the idea..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The CRS is: \", cru_arctic_ts[1901].dataset.rio.crs)\n",
    "print(\"Time dimension shape: \", cru_arctic_ts[1901].dataset.time.shape)\n",
    "print(\"The data variables present: \\n\", cru_arctic_ts[1901].dataset.data_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2,1)\n",
    "tmax_im = axes[0].imshow(cru_arctic_ts[start_year].dataset['tmax'][0])\n",
    "pre_im = axes[1].imshow(cru_arctic_ts[start_year].dataset['pre'][0], cmap='Greens')\n",
    "c1 = plt.colorbar(tmax_im, ax=axes[0])\n",
    "c2 = plt.colorbar(pre_im, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Clip all files down to tile extents\n",
    "\n",
    "Now that we have this data loaded (and resampled), we can clip out the data for our tile. For this we use the \"`Tile.import_normalized(...)` function. \n",
    "\n",
    "> This is **SLOW** (40 minutes on 8 core 32GB RAM laptop; I think it fills the RAM and then slows way down using the swap memory...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### serial\n",
    "mytile.verbose = True\n",
    "mytile.import_normalized('cru_AnnualTimeSeries', cru_arctic_ts)\n",
    "\n",
    "### Parallel\n",
    "\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# import joblib\n",
    "\n",
    "# # replace with whichever cluster class you're using\n",
    "# # https://docs.dask.org/en/stable/deploying.html#distributed-computing\n",
    "# cluster = LocalCluster(n_workers=24) # <<< change if needed\n",
    "# # connect client to your cluster\n",
    "# client = Client(cluster)\n",
    "# print(client.dashboard_link)\n",
    "\n",
    "# start = datetime.now()\n",
    "# with joblib.parallel_config(backend=\"dask\", n_jobs=20, verbose=1):\n",
    "#     mytile.import_normalized('cru_AnnualTimeSeries', cru_arctic_ts, parallel=True)\n",
    "\n",
    "\n",
    "### end parallel\n",
    "\n",
    "#del(cru_arctic_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Great! Now we have a tile with all the data loaded. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mytile.data.keys())\n",
    "print(mytile.data['worldclim'].dims)\n",
    "\n",
    "cru_first_year = mytile.data['cru_AnnualTimeSeries'][start_year]\n",
    "cru_last_year = mytile.data['cru_AnnualTimeSeries'][end_year]\n",
    "print(cru_first_year.dataset.dims)\n",
    "print(cru_last_year.dataset.dims)\n",
    "\n",
    "print(cru_first_year.dataset.rio.crs)\n",
    "print(mytile.data['worldclim'].rio.crs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Saving the `Tile`\n",
    "\n",
    "The directory is created based on the tile's index (horizontal and vertical, i.e.: H00_V08)\n",
    "\n",
    "> **SLOW**, maybe 50 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile.toggle_verbose()\n",
    "mytile.save('working/03-tiles', overwrite=True, clear_existing=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Extra Stuff (plots, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mytile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f\"{wc['x'].min().values}\")\n",
    "print(f\"{wc['x'].max().values}\")\n",
    "print(f\"{wc['y'].min().values}\")\n",
    "print(f\"{wc['y'].max().values}\")\n",
    "print(bounds)\n",
    "wc_bnds = pd.DataFrame({'minx': [wc['x'].min().values], 'maxx': [wc['x'].max().values],\n",
    "                        'miny': [wc['y'].min().values], 'maxy': [wc['y'].max().values]})\n",
    "print(wc_bnds)\n",
    "print(wc_bnds['minx'][0] - bounds['minx'].values[0])\n",
    "print(wc_bnds['maxx'][0] - bounds['maxx'].values[0])\n",
    "print(wc_bnds['miny'][0] - bounds['miny'].values[0])\n",
    "print(wc_bnds['maxy'][0] - bounds['maxy'].values[0])\n",
    "\n",
    "print(wc['x'][0]-wc['x'][1]) # 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].plot(mytile.data['worldclim']['x'])\n",
    "axes[0].plot(mytile.data['cru_AnnualTimeSeries'][start_year].dataset['x'])\n",
    "axes[1].plot(mytile.data['worldclim']['y'])\n",
    "axes[1].plot(mytile.data['cru_AnnualTimeSeries'][start_year].dataset['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
