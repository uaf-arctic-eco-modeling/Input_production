{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will show the following operations for a `Tile` object:\n",
    " \n",
    " - Creating a tile object from a shape file.\n",
    " - Loading data into the tile object for both world clim and cru-jra data.\n",
    " - Saving the tile object to a folder.\n",
    "\n",
    "When a tile is created, it is empty, but has an extent as determined from the \n",
    "shape file used to create the tile. \n",
    "\n",
    "When loading data, it is assumed that the world clim and cru data sources have\n",
    "extents fully encompasing the tile. When the data is loaded it is clipped to the\n",
    "extents of the tile, optionally including a buffer beyond the tile extents\n",
    "(reccomened; helps with warping and reprojection).\n",
    "\n",
    "When a tile is saved, all the datasets that the tile holds are saved to disk in \n",
    "an appropriately dimensioned netcdf file. Additionally there is a manifest file\n",
    "which will have metadata (tile index, extents, resolution, crs, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development...\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from temds.datasources import dataset, timeseries\n",
    "\n",
    "from temds import tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Set up a log for the process\n",
    "\n",
    "Our tools use an object to log and display various messages that may be created. There are several message priorities that\n",
    "a logged message may have. All messages are saved, but only messages with the priority set in `verbose_levels` are printed to the screen\\console. For this demo we will display all **INFO** messages, but if you are having issues it may be usefull to \n",
    "log all **DEBUG** messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from temds import logger\n",
    "log = logger.Logger(verbose_levels=logger.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Creating a `Tile` object\n",
    "\n",
    "Here we setup a few parameters defining the tile we want to create. This includes\n",
    "the:\n",
    "  - Tile \"index\" (horizontal and vertical offsets from lower left corner). \n",
    "  The shape file should be annotated with these horizontal and vertical indices \n",
    "  at attributes for each tile.\n",
    "  - Year range to work with (typically all the data you have, but for \n",
    "    development and testing you can make it shorter).\n",
    "\n",
    "Then we read the bounds for the tile from the shape file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1901\n",
    "end_year = 2023\n",
    "c_tile = (0, 8)\n",
    "tile_index = gpd.read_file('working/00-aoi/tile-index-annotated/')\n",
    "hdx = tile_index['H'] == c_tile[0]\n",
    "vdx = tile_index['V'] == c_tile[1]\n",
    "bounds = tile_index[vdx & hdx].bounds\n",
    "minx, maxx, miny, maxy = bounds[['minx','maxx','miny','maxy']].iloc[0]\n",
    "minx, maxx, miny, maxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Next we create a tile object. At this point the tile object is \"empty\" - there is no data loaded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile = tile.Tile(c_tile, bounds, 4000, tile_index.crs, buffer_px=20, logger=log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile.buffer_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytile.extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Loading data to the `Tile` object\n",
    "\n",
    "Once we have created a `Tile` object we can load some data to it. \n",
    "\n",
    "Before we can import (load) the data into the `Tile` object, we need to open and create the datasets. For this tile, we will be using WorldClim and CRU-JRA data.\n",
    "\n",
    "## WorldClim\n",
    "\n",
    "The WorldClim data is relatively straightforward - it is a single NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_arctic = dataset.TEMDataset('/media/rwspicer/data/V3/tem/02-arctic/worldclim/worldclim-arctic.nc')\n",
    "\n",
    "print(f\"The CRS for the WorldClim dataset is: {wc_arctic.crs}\")\n",
    "\n",
    "mytile.import_and_normalize('worldclim', wc_arctic)\n",
    "\n",
    "mytile.data['worldclim']\n",
    "#del(wc_arctic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"WorldClim dimensions: {mytile.data['worldclim'].dataset.dims}\")\n",
    "print(f\"Resolution: {mytile.resolution}\")\n",
    "tile_x_size = (mytile.extent['maxx']-mytile.extent['minx'])/mytile.resolution\n",
    "tile_y_size = (mytile.extent['maxy']-mytile.extent['miny'])/mytile.resolution\n",
    "print(f\"Tile Size: {tile_x_size.values} x {tile_y_size.values} pixels\")\n",
    "print(f\"Tile Buffer Pixels: {mytile.buffer_pixels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now we can plot and look at the data to double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots (1,1, dpi=100)\n",
    "\n",
    "im = axes.imshow(mytile.data['worldclim'].dataset['tair_max'].data[0], origin='lower')\n",
    "fig.colorbar(im, ax=axes)\n",
    "axes.set_title('wc example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## CRU-JRA\n",
    "The CRU-JRA data is a little more complicated as it is a series of NetCDF files.\n",
    "\n",
    "First we can do some poking around to see what files we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = sorted(list(Path('working/02-arctic/cru-jra-standard/').glob('*.nc')))\n",
    "print(f\"Found {len(i)} files.\")\n",
    "print(i[0])\n",
    "print(\"...\")\n",
    "print(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "It is up to the user to make sure they have enough files. You must have a continuous range covering your `start_year` through your `end_year` that you set above.\n",
    "\n",
    "> (~4 minutes and 18GB RAM on 8 core machine with 32GB RAM and SSD)\n",
    "\n",
    "### Load up the CRU-JRA arctic files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens cru files, reads into memory, then we have a list of cru objects at \n",
    "# the end...\n",
    "files = Path('working/02-arctic/cru-jra-standard/')\n",
    "\n",
    "log.suspend()\n",
    "cru_arctic = timeseries.YearlyTimeSeries(files, logger=log, in_memory=False) ## IF you have alot of memory set in_memory=True\n",
    "log.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> If you pass `in_memory=False` to the `Timeseries` or 'TEMDataset` constructor, you will get\n",
    "> a \"lazy loading\" object, where the `._dataset` attribute holds a path instead\n",
    "> of an xarray object!! E.g. \n",
    "> \n",
    "> ```\n",
    "> >>> print(cru_arctic[1901]._dataset\n",
    "> PosixPath('working/02-arctic/cru-jra-fixed/crujra.arctic.v2.5.5d.1901.365d.noc.nc')\n",
    "> ```\n",
    "\n",
    "\n",
    "We can plot this data to confirm that it is the expected shape. The data should be monthly and have all variables present. The data should cover the entire acrtic and is un-projected, so it doesn't look great, but you get the idea..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The CRS is: \", cru_arctic[1901].crs)\n",
    "print(\"Time dimension shape: \", cru_arctic[1901].dataset.time.shape)\n",
    "print(\"The data variables present: \\n\", cru_arctic[1901].vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2,1)\n",
    "tmax_im = axes[0].imshow(cru_arctic[start_year].dataset['tair_max'][0])\n",
    "pre_im = axes[1].imshow(cru_arctic[start_year].dataset['prec'][0], cmap='Greens')\n",
    "c1 = plt.colorbar(tmax_im, ax=axes[0])\n",
    "c2 = plt.colorbar(pre_im, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Clip all files down to tile extents\n",
    "\n",
    "Now that we have this data loaded (and resampled), we can clip out the data \n",
    "for our tile. For this we use the \"`Tile.import_normalized(...)` function. \n",
    "\n",
    "> This is a memory hog! Doing years 1901-2023 uses ~30GB RAM and takes about \n",
    "> 20 min..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### serial\n",
    "mytile.import_and_normalize('cru-intput', cru_arctic, clip_with='gdal', parallel=False)\n",
    "\n",
    "### Parallel\n",
    "\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# import joblib\n",
    "\n",
    "# # replace with whichever cluster class you're using\n",
    "# # https://docs.dask.org/en/stable/deploying.html#distributed-computing\n",
    "# cluster = LocalCluster(n_workers=24) # <<< change if needed\n",
    "# # connect client to your cluster\n",
    "# client = Client(cluster)\n",
    "# print(client.dashboard_link)\n",
    "\n",
    "# start = datetime.now()\n",
    "# with joblib.parallel_config(backend=\"dask\", n_jobs=20, verbose=1):\n",
    "#     mytile.import_normalized('cru_AnnualTimeSeries', cru_arctic_ts, parallel=True)\n",
    "\n",
    "\n",
    "### end parallel\n",
    "\n",
    "#del(cru_arctic_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Great! Now we have a tile with all the data loaded. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mytile.data.keys())\n",
    "print(mytile.data['worldclim'].dims)\n",
    "\n",
    "cru_first_year = mytile.data['cru-input'][start_year]\n",
    "cru_last_year = mytile.data['cru-input'][end_year]\n",
    "print(cru_first_year.dataset.dims)\n",
    "print(cru_last_year.dataset.dims)\n",
    "\n",
    "print(cru_first_year.dataset.rio.crs)\n",
    "print(mytile.data['worldclim'].rio.crs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Saving the `Tile`\n",
    "\n",
    "The directory is created based on the tile's index (horizontal and vertical, i.e.: H00_V08)\n",
    "\n",
    "> Maybe 50 minutes with default compression level (9)\n",
    "> Compression level 1 only takes 10 minutes and results in ~16GB of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.suspend()\n",
    "mytile.save('working/03-tiles', overwrite=True, clear_existing=True, use_zlib=False)\n",
    "log.resume()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Extra Stuff (plots, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mytile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f\"{wc['x'].min().values}\")\n",
    "print(f\"{wc['x'].max().values}\")\n",
    "print(f\"{wc['y'].min().values}\")\n",
    "print(f\"{wc['y'].max().values}\")\n",
    "print(bounds)\n",
    "wc_bnds = pd.DataFrame({'minx': [wc['x'].min().values], 'maxx': [wc['x'].max().values],\n",
    "                        'miny': [wc['y'].min().values], 'maxy': [wc['y'].max().values]})\n",
    "print(wc_bnds)\n",
    "print(wc_bnds['minx'][0] - bounds['minx'].values[0])\n",
    "print(wc_bnds['maxx'][0] - bounds['maxx'].values[0])\n",
    "print(wc_bnds['miny'][0] - bounds['miny'].values[0])\n",
    "print(wc_bnds['maxy'][0] - bounds['maxy'].values[0])\n",
    "\n",
    "print(wc['x'][0]-wc['x'][1]) # 4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].plot(mytile.data['worldclim']['x'])\n",
    "axes[0].plot(mytile.data['cru-input'][start_year].dataset['x'])\n",
    "axes[1].plot(mytile.data['worldclim']['y'])\n",
    "axes[1].plot(mytile.data['cru-input'][start_year].dataset['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
